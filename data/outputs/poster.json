{
  "title": "Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum Hadi Pouransari1,◦",
  "authors": [
    "Hadi Pouransari1,◦",
    "Vaishaal Shankar2,†"
  ],
  "layout": {},
  "sections": [
    {
      "title": "Abstract",
      "text": "Large language models (LLMs) are commonly trained on datasets consisting of\nfixed-length token sequences. These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a\npredetermined target length (concat-and-chunk). Recent attention implementations\nmask cross-document attention, reducing the effective length of a chunk of tokens.\nAdditionally, training on long sequences becomes computationally prohibitive due\nto the quadratic cost of attention. In this study, we introduce dataset decomposition,\na novel variable sequence length training technique, to tackle these challenges. We\ndecompose a dataset into a union of buckets, each containing sequences of the same\nsize extracted from a unique document. During training, we use variable sequence\nlength and batch-size, sampling simultaneously from all buckets with a curriculum.\nIn contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at\nevery step of training, our proposed method incurs a computational cost proportional to the actual document lengths at each step, resulting in significant savings\nin training time. We train an 8k context-length 1B model at the same cost as a 2k\ncontext-length model trained with the baseline approach. Experiments on a webscale corpus demonstrate that our approach significantly enhances performance\non standard language evaluations and long-context benchmarks, reaching target\naccuracy with up to 6× faster training compared to the baseline. Our method not\nonly enables efficient pretraining on long sequences but also scales effectively with\ndataset size. Lastly, we shed light on a critical yet less studied aspect of training\nlarge language models: the distribution and curriculum of sequence lengths, which\nresults in a non-negligible difference in performance.*\n1",
      "start_page": null,
      "end_page": null,
      "images": [],
      "metadata": {
        "bullets": [
          "Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences.",
          "These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a predetermined target length (concat-and-chunk).",
          "In this study, we introduce dataset decomposition, a novel variable sequence length training technique, to tackle these challenges.",
          "We decompose a dataset into a union of buckets, each containing sequences of the same size extracted from a unique document.",
          "We train an 8k context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach.",
          "Experiments on a webscale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy with up to 6× faster training compared to the baseline."
        ]
      }
    },
    {
      "title": "Introduction",
      "text": "Large language models (LLMs) are often pretrained autoregressively (i.e., predicting the next token\ngiven a context) on large text corpora sourced from the web. Examples include The Pile ,\nRefinedWeb , RedPajama , and DOLMA . Each of these datasets comprises multiple\ndocuments, ranging from Wikipedia articles to books and code repositories. While the individual\nlengths of the documents vary from a few words (e.g., a message) to hundreds of thousands of words\n(e.g., a book), the training infrastructure often supports only a limited sequence length in a batch.\nTo facilitate efficient training, document chunking is necessary. In this paper, we investigate the\ninfluence of document chunking, propose alternative strategies, and evaluate the proposed strategies\nwith careful experiments.\n◦Corresponding author: mpouransari@apple.com, †Work is done when at Apple.\n*Code to be available at https://github.com/apple/ml-dataset-decomposition.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n\n\n[[PAGE 2]]\n233\n234\n235\n236\n237\n238\n239\n240\n# of seen tokens\n40\n45\nRegular eval (%)\nBaseline-8k\nDataset Decomposition\n>4⨉ data\nefﬁciency\n+2.4\n(a) Data Efficiency\n0\n1000\n2000\n3000\n4000\n5000\nGPU-Hours\n40\n45\nRegular eval (%)\nBaseline-8k\nDataset Decomposition\n>6⨉ training speed-up\n(b) Computational Efficiency\nFigure 1: Scaling the training of the OpenLM-410M model to 1.1 trillion tokens on the RefinedWeb\ndataset. Note that each point on the figure represents a separate training from scratch (not different\ncheckpoints of a single run). For the largest run, some tokens are seen more than once (the dataset\nhas approximately 525 billion tokens). For dataset decomposition, we use the Grow-P2 curriculum\nwith 8 cycles. Maximum context length is 8192 and all hyper-parameters are the same for DD and\nthe baseline. (a) Regular metrics average when training with the baseline method and the proposed\nmethod. We observe more than 4× data efficiency compared to the baseline. Also, even at 1.1 trillion\ntokens, DD has a +2.4 accuracy improvement compared to the baseline (which has a plateauing\naccuracy curve even on a logarithmic x-axis). (b) Comparison of model average accuracy versus\ntraining cost (GPU-hours). DD reaches the best accuracy of the baseline more than 6× faster. This is\nthe combined effect of DD accuracy and speed gains.\nRecent works  popularized the concat-and-chunk approach to convert text datasets with\nvariable document lengths into sequences with a fixed target length. In this approach, during a data\npreparation stage before training, we first randomly shuffle and concatenate all tokenized documents.\nConsecutive concatenated documents are separated by a special token <EOT>, allowing the model\nto detect document boundaries. We then chunk the concatenated sequence into subsequences with\na target sequence length. For example, 2048 and 4096 for the Llama-1 and Llama-2 models,\nrespectively. The model is then pretrained on batches of sequences with fixed length.\nThe concat-and-chunk approach has several shortcomings. First, randomly concatenating documents\ncan lead to the model attending to a context from an unrelated document to predict the next token.\nWhile well-trained models learn to avoid cross-document attention, this is not explicitly enforced,\nleading to potential spurious modeling. Second, the cross-document attention spends unnecessary\ncomputation on attending to unrelated tokens that do not facilitate learning. This is especially crucial\ndue to the quadratic complexity of the attention mechanism. Even with an implementation of attention\nthat supports cross-document attention masking, the computational cost for each optimization step\nwould be bottlenecked by the longest document in the global batch, leading to significant underutilization of devices with shorter documents. Third, even if a document is shorter than the target\nsequence length, it may still be broken into two chunks when they are at the boundary of two\nsequences. This results in significantly smaller average chunk lengths compared to the original\ndocument length average (see Fig. 3a), which hinders the model’s capability.\nRecent and concurrent works on LLM training try to improve the concat-and-chunk approach:\ndocument-masking is possible with recent implementation of attention  as adopted in some recent\npre-training recipes , best-fit packing  to reduce document chunking, and concatenating\nsemantically related documents instead of randomly . However, none of them address all three\nissues mentioned above together.\nIn this work, we introduce dataset decomposition (DD), a novel approach to decompose data based\non their length and train with variable sequence length (VSL) and length-based curriculum to address\nthe above issues. We obtain significant both significant accuracy improvement and straining speed-up\nas shown in Fig. 1. DD decomposes a given dataset containing documents of variable lengths into\na union of datasets/buckets, each with sequences of a fixed length. Specifically, a dataset D is\ndecomposed into buckets ∪iDi, where each bucket Di contains sequences of length 2i, each extracted\nfrom a unique document. During training with VSL, at every step of the optimization process, we\nsample i (based on a curriculum) to form a batch with b/2i sequences from the bucket Di, which\n2\n\n\n[[PAGE 3]]\nconcat-and-chunk\noriginal documents\ndataset decomposition\n<latexit sha1_base64=\"nFp3aHz68n1OhxJhj1CLZTrzGPo=\">AB/nicbVDLSsNAFJ3UV42vqLhyM9gIrkrSRXVZ1IXLCvY\nBbQiT6aQdOpmEmYlQsBfceNCEbd+hzv/xkmbhbYeGDicy/3zAkSRqVynG+jsra+sblV3TZ3dvf2D6zDo6MU4FJB8csFv0AScIoJx1FSP9RBAUBYz0gulN4fceiZA05g9qlhAvQmNOQ4qR0pJvndi2OYyQmDEstvcz9zctG3fqjl1Zw64StyS1ECJtm9DUcxT\niPCFWZIyoHrJMrLkFAUM5Kbw1SBOEpGpOBphxFRHrZPH4Oz7UygmEs9OMKztXfGxmKpJxFgZ4sksplrxD/8wapCq+8jPIkVYTjxaEwZVDFsOgCjqgWLGZJgLqrNCPECYaUbM3UJ7vKXV0m3UXeb9eZ9o9a6LuoglNwBi6ACy5BC9yBNugADLwDF7Bm/FkvBj\nvxsditGKUO8fgD4zPH9mlCQ=</latexit>D1\n<latexit sha1_base64=\"7SO8TkJnM4K89i5X4tkANgL5Kvk=\">AB/nicbVDLSsNAFJ3UV42vqLhyM9gIrkrSRXVZ1IXLCvY\nBbQiT6aQdOpmEmYlQsBfceNCEbd+hzv/xkmbhbYeGDicy/3zAkSRqVynG+jsra+sblV3TZ3dvf2D6zDo6MU4FJB8csFv0AScIoJx1FSP9RBAUBYz0gulN4fceiZA05g9qlhAvQmNOQ4qR0pJvndi2OYyQmDEstvczxq5adu+VXPqzhxwlbglqYESbd/6Go5in\nEaEK8yQlAPXSZSXIaEoZiQ3h6kCcJTNCYDTmKiPSyefwcnmtlBMNY6McVnKu/NzIUSTmLAj1ZJXLXiH+5w1SFV5GeVJqgjHi0NhyqCKYdEFHFBsGIzTRAWVGeFeIEwko3ZuoS3OUvr5Juo+426837Rq1XdZRBafgDFwAF1yCFrgDbdABGTgGbyCN+PJeDH\nejY/FaMUod47BHxifP9stlCU=</latexit>D2\n<latexit sha1_base64=\"yCfMh6vCIQCzQ7VEb65tp8Zwcbk=\">AB/nicbVDLSsNAFJ34rPEVFVduBhvBVUkqVJdFXbisYB/\nQhjCZTtqhk0mYmQglBPwVNy4Ucet3uPNvnLRZaOuBgcM593LPnCBhVCrH+TZWVtfWNzYrW+b2zu7evnVw2JFxKjBp45jFohcgSRjlpK2oYqSXCIKigJFuMLkp/O4jEZLG/EFNE+JFaMRpSDFSWvKtY9s2BxFSY4xYdpv72UVu2rZvVZ2aMwNcJm5JqBEy7e+BsMYp\nxHhCjMkZd91EuVlSCiKGcnNQSpJgvAEjUhfU4iIr1sFj+HZ1oZwjAW+nEFZ+rvjQxFUk6jQE8WSeWiV4j/ef1UhVdeRnmSKsLx/FCYMqhiWHQBh1QrNhUE4QF1VkhHiOBsNKNmboEd/HLy6RTr7mNWuO+Xm1el3VUwAk4BefABZegCe5AC7QBhl4Bq/gzXgyXox\n342M+umKUO0fgD4zPH9y1lCY=</latexit>D3\nFigure 2: Each cell in the figure represents a token. Left: Original documents with variable lengths.\nMiddle: Concat-and-chunk baseline to form sequences with a fixed target length (here = 4). Right:\nDataset decomposition method with D1, D2, and D3 buckets .\nkeeps the total number of tokens in a batch constant (2i × b/2i = b), regardless of which Di is\nsampled.\nThis approach gives us several advantages and resolves the aforementioned issues of the concatand-chunk method. First, DD is simple and has negligible computational overhead during the data\npreparation stage, making it easy to scale to large datasets. Second, tokens in each sequence are\nensured to be from the same document by construction, which avoids cross-document attention.\nFurthermore, we have access to the sequence length distribution (an auxiliary prior knowledge) which\ncan be used to create different mixtures/curricula for training. Finally, our VSL training strategy\naccelerates training time: the latency for one optimization step is less when sampling from Di with\nsmaller i (due to attention’s quadratic complexity). Following is a summary of our contributions:\n• We introduce DD, a method to efficiently decompose a dataset of variable-length documents into a\nunion of buckets with fixed-length sequences. DD enables efficient and robust training via VSL\nand length-based curriculum.\n• We perform large-scale experimentation using different models, datasets, and evaluation tasks to\ndemonstrate the efficacy of the proposed method. We show (see Fig. 1) significant gains in data\nefficiency (> 4×) and compute efficiency (11% to 45%), resulting in combined LLM pretraining\nacceleration of up to 6× (time to reach certain accuracy compared to baseline).\n• Through careful experimentation, we study the importance of sequence length distribution and\nmixture during pretraining for different natural language and long-context tasks. We show the effect\nof concatenation and chunking operations to synthetically alter sequence length (Section 3.2).\n2",
      "start_page": 2,
      "end_page": 2,
      "images": [],
      "metadata": {
        "bullets": [
          "Large language models (LLMs) are often pretrained autoregressively (i.e., predicting the next token given a context) on large text corpora sourced from the web.",
          "Each of these datasets comprises multiple documents, ranging from Wikipedia articles to books and code repositories.",
          "In this paper, we investigate the influence of document chunking, propose alternative strategies, and evaluate the proposed strategies with careful experiments.",
          "*Code to be available at https://github.com/apple/ml-dataset-decomposition.",
          "38th Conference on Neural Information Processing Systems (NeurIPS 2024).",
          "[[PAGE 2]] 233 234 235 236 237 238 239 240 # of seen tokens 40 45 Regular eval (%) Baseline-8k Dataset Decomposition >4⨉ data efﬁciency +2.4 (a) Data Efficiency 0 1000 2000 3000 4000 5000 GPU-Hours 40 45 Regular eval (%) Baseline-8k Dataset Decomposition >6⨉ training speed-up (b) Computational Effic"
        ]
      }
    },
    {
      "title": "results?",
      "text": "Answer: [Yes]\nJustification: We provide all implementation details in Appendix B.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment Statistical Significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: Training LLM at scale is computationally expensive. We repeat all experiments\nat 17B total tokens scale twice (with different random seeds) and report mean and variance\nin Section 3.2. For efficiency benchmarks we repeat 5 times, report standard deviation in\nFig. 7.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments Compute Resources\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\n24\n\n\n[[PAGE 25]]\nAnswer: [Yes]\nJustification: We provide software/hardware details in Appendix B.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: All experiments fully respect NeurIPS code of ethics.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\n10. Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: We discuss broader impacts of this work in Appendix A.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: We do not believe the models/code related to this work poses any risk that\nrequires safeguarding.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n25\n\n\n[[PAGE 26]]\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We do our best to appropriately cite/acknowledge all external code/data used\nin this work.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the package\nshould be provided. For popular datasets, paperswithcode.com/datasets has\ncurated licenses for some datasets. Their licensing guide can help determine the license\nof a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: Aside from code, this work does not intend to release any asset.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: This work does not include any crowdsourcing or research with Human\nsubjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n26\n\n\n[[PAGE 27]]\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: Not applicable to this work.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n27",
      "start_page": 25,
      "end_page": 25,
      "images": [],
      "metadata": {
        "bullets": [
          "Answer: [Yes] Justification: We provide all implementation details in Appendix B.",
          "Guidelines: • The answer NA means that the paper does not include experiments.",
          "• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.",
          "7.",
          "Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?",
          "We repeat all experiments at 17B total tokens scale twice (with different random seeds) and report mean and variance in Section 3.2."
        ]
      }
    },
    {
      "title": "Method",
      "text": "2.1\nDataset decomposition\nGiven a dataset D of tokenized documents {d1, d2, . . . , dn}, the goal of dataset decomposition (DD)\nis to reorganize D as a union of buckets, ∪iDi, such that: (1) each bucket Di consists of sequences\nof tokens with length li; (2) each sequence s ∈Di is a subsequence of one document d ∈D; and\n(3) each token in D appears in exactly one Di. This decomposition produces sequences that each\nbelong to a unique document, ensuring no cross-document attention within a sequence during training.\nAdditionally, all sequences in a given bucket Di have the same length li, enabling efficient batching.\nDataset decomposition as defined above is not unique. We propose a specific decomposition, with\nli = 2i, to optimally maintain the original document sequence length distribution while also enabling\nefficient batch pretraining, as explained in Section 2.2. We apply decomposition at the document\nlevel, which makes it very easy to integrate the method into any existing data preparation pipeline\n(a stage before model training) and is scalable to large datasets. For a tokenized document d ∈D\nwith length l, where l = 2i1 + 2i2 + . . . + 2ik represents its binary decomposition, we break d into k\nadjacent sequences s1, . . . , sk, with lengths of 2i1, . . . , 2ik, respectively. Each sequence sj of length\n2ij is then assigned to bucket Dij. Fig. 2 shows a schematic representation of this method.\nWith our proposed dataset decomposition approach, each bucket Di contains sequences extracted\nfrom an original document d such that the length of d is at least 2i. In Fig. 3b, we show the distribution\n3\n\n\n[[PAGE 4]]\n25\n26\n27\n28\n29\n210 211 212 213 214\nSequence length\n10−7\n10−6\n10−5\n10−4\n10−3\n10−2\n10−1\nPDF\n3.8%\n0.3%\n0.6%\nOrig. Docs\nmean=598\nBaseline-2k\nmean =463\nBaseline-8k\nmean =558\nPack-8k\nmean =585\n(a)\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17\nBucket number\n2\n4\n6\n8\n10\n12\n14\n16\nTokens %\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n(b)\n25\n26\n27\n28\n29\n210\n211\n212\n213\nContext length\n10−5\n10−4\n10−3\nPDF\nBaseline-8k\nmean=930\nPack-8k\nmean=1064\nDD≥256\nmean=1344\n(c)\nFigure 3: For the RefinedWeb dataset : (a) Distribution of chunk lengths using different dataset\npreparation methods. Peaks show the percentage of chunks for each method with the same length\nas the target sequence length. (b) Distribution of tokens over Di’s in DD. Color/pattern shows the\n⌊log2 l⌋, where l is the length of the original document each token is extracted from. (c) Probability\ndistribution of context length (number of tokens from the same document a token can attend to)\nobserved during training for the concat-and-chunk baseline with target sequence length 8192 and DD\nwith ≥256 mixture defined in Table 1.\nof RefinedWeb dataset tokens over different buckets, where D9 (corresponding to sequences with\nlength 512) has the maximum tokens. We also highlight the original document lengths from which\ntokens are extracted. Most tokens in a bucket Di are extracted from documents with length l such\nthat 2i ≤l < 2i+1, and some tokens are rolled over from documents with length l ≥2i+1. This\ndemonstrates the efficacy of the method in retaining original document length, especially for long\ndocuments, which are scarce.\nIn Fig. 3a, we show the distribution of original document lengths and chunks within 2048 and\n8192 target sequence lengths formed by the concat-and-chunk approach. We also present the length\ndistribution using the bin-packing approximate algorithm introduced by a concurrent work .\nAdditionally, in Fig. 3c, we show the distribution of context length (the number of tokens from the\nsame document a token can attend to during pretraining) when using baselines with a target sequence\nlength of 8192 and DD. See Appendix F for additional discussion on sequence length statistics.\nIn contrast to the concat-and-chunk approach, which results in a static dataset, DD enables us to use\nsequence length distribution as prior knowledge and optimize the best mixture for the target task. In\nSection 3.2, we show the bias of each target evaluation toward a sequence length and the effect of\nconcatenation and chunking on model performance. In Section 3.3, we study the effect of different\nsequence mixtures for LLM pretraining, a less-studied topic in LLM pretraining.\n2.2\nVariable sequence length training\nFollowing the setup in Section 2.1, we assume a set of k buckets such that Di, containing sequences\nwith length 2i, are available. Let b be the target batch size - the number of tokens used per optimization\nstep. In variable sequence length (VSL) training, at every step of optimization, we first sample i from\navailable choices, then pick b/2i sequences from bucket Di. Since Di consists of sequences with\nlength 2i, the number of seen tokens per optimization step remains b, independent of the choice of i.\nTraining LLMs with the VSL algorithm comes with several advantages.\nFirst, since the total number of seen tokens per optimization step does not change, VSL does not alter\noptimization dynamics, and the same hyperparameters as the baseline can be utilized (see Section 3).\nSecond, in Section 3.1, we show that the time to complete one optimization step (forward+backward)\nfor a fixed b (tokens per step) varies by sequence length due to the quadratic cost of attention .\nWith VSL training, the cost of every optimization step depends on the bucket Di sampled for that step\n(and hence the sequence length). Thus, the more expensive steps (corresponding to long sequences)\nare compensated with less expensive steps (corresponding to short sequences).\n4\n\n\n[[PAGE 5]]\n26\n27\n28\n29\n210\n211\n212\n213\nContext length\n200\n400\n600\n800\n1000\nOne step training latency (ms)\n+35%\n+88%\n+23%\nOpenLM-1B\nOpenLM-3B\nOpenLM-7B\n(a)\n(b)\nFigure 4: (a) Average time for one optimization step (b = 8 × 8192 tokens) on an 8×H100 node with\nFSDP and FlashAttention2 for different context lengths. (b) OpenLM-1B/3B/7B models trained on\n137B tokens. Accuracy and training speed gains are shown.\nFinally, the sampling component in VSL (which Di to choose at every optimization step) enables\ndifferent curricula of sequence lengths. In Section 3.4, we show the significance of such curricula on\nmodel stability and generalization accuracy.\n3\nExperiments and analysis\nIn this section, we show the efficacy of the proposed method to train LLMs of different sizes on largescale datasets and provide additional analyses. For all experiments, except the results in Section 3.5,\nwe use RefinedWeb  filtering of Common Crawl  with a total of ∼525 billion tokens using the\nEleutherAI/gpt-neox  tokenizer (vocabulary size is 50,432). Model architectures and training code\nare based on the OpenLM †. For all experiments, other than model scaling in Section 3.5, we use\nthe OpenLM-1B model with an 8k context length. Please refer to Appendix B for implementation\ndetails of all experiments.\nPositional encoding\nWe use Rotary Positional Embedding (RoPE)  to encode positions in\nqueries and keys before the attention module. RoPE rotates the consecutive components of queries\nand keys with a base frequency fb = 10, 000. Recent studies  have suggested increasing\nfb to better adapt a pretrained model for longer sequences through fine-tuning. We find that using a\nlarger fb is also beneficial when training LLMs from scratch. In Table 4, we show that increasing fb\nto 100,000 improves performance for both the baseline and DD methods.\nEvaluation\nWe evaluate each model on a comprehensive set of standard benchmarks, mainly using\nLLM Foundry . We report averaged accuracies over each category, as well as the regular average,\nwhich is the average of 14 regular language modeling benchmarks detailed below:\n• Commonsense Reasoning (CSR): PIQA-0-shot , COPA-0-shot , and OpenBookQA-10shots .\n• Language Understanding (LU): Lambada-OpenAI , Hellaswag-0-shot , Winograd-3shots , and WinoGrande-5-shots .\n• Reading Comprehension (RC): SQuAD-3-shots , BoolQ-0-shot , and CoQA-0-shot .\n• World Knowledge (WK): Jeopardy-3-shots , ArcEasy-3-shots , ArcChallenge-3shots , and WikiDataQA-3-shots\nTo evaluate model on longer context tasks, we adopt the following real-world benchmarks:\n• Multi-Document Question Answering (MDQA): We follow the exact setup as in Liu et al. ,\nwhere for each question from NaturalQuestions-Open , r Wikipedia documents are retrieved\nsuch that one of them has the answer to the question, and the other r −1 documents are distractors.\nWe report MDQA-10, MDQA-20, and MDQA-30 accuracy corresponding to r = 10, 20, and 30,\n†https://github.com/mlfoundations/open_lm\n5\n\n\n[[PAGE 6]]\n(a)\n(b)\n(c)\nFigure 5: (a) Performance of OpenLM-1B model trained on 234 tokens from buckets with different\nsequence lengths. (b) distribution of lengths of documents for different benchmarks. (c) Effect of\nchunking (D13→10) and concatenating (D7→13) sequences during pretraining on model performance.\nrespectively. For each query, we evaluate the model by changing the location of the target document\namong distractors and report the averaged accuracy.\n• TOEFL: This dataset is a multiple-choice question answering dataset from An et al. . The\ndataset contains QA pairs for 15 longest lectures in Tseng et al. , Chung et al. . Only one of\nthe choices is the correct response. We estimate the correct choice by picking the choice with the\nlowest mean log probability value.\n• QuALITY: This dataset is a multiple-choice question answering dataset from An et al. . The\ndataset contains a long passage for context, followed by a question with multiple choices. Only one\nof the choices is the correct response. We estimate the correct choice by picking the choice with the\nlowest mean log probability value.\n3.1\nTraining efficiency\nWe first verify that VSL training enables a higher throughput than the baseline concat-and-chunk\nmethod. We enumerate model sizes (OpenLM-1B/3B/7B) and different context lengths (26 to 213)\nand measure the time to train 100 batches with a fixed global batchsize of b = 8 × 8192 distributed\nover 8 GPUs in a single node. We repeat this 5 times and report the average time per optimization step\nin Fig. 4a (with STD mostly < 1ms). See Appendix C.1 for additional results with different batchsizes\nb. For each model, we highlight the training time overhead (due to attention’s quadratic complexity\nwith an optimized FlashAttention2 kernel ) when training with 8192 context lengths compared to\n64 context lengths: +35%, +88%, and +23% for OpenLM-1B, -3B‡, and -7B, respectively. Training\noverhead grows for longer context lengths (see Fig. 7 for results up to 16k context length).\nThe concat-and-chunk baseline method always operates at a fixed sequence length. For example, for\nthe OpenLM-1B model, an optimization step with concat-and-chunk takes 243ms and 304ms for\ntarget context lengths of 2048 and 8192, respectively. The expected time for VSL, on the other hand,\nis the weighted average over different sequence lengths depending on the mixture. In Table 1, we\nreport the training step time for different mixtures. For example, with the natural length distribution\nresulting from DD (Fig. 3b), training up to length 8192 sequences takes a similar time (244ms) as\nbaseline training with length 2048 (with 243ms per step) per step-equivalent to a 20% training time\nreduction compared to baseline training with a fixed length of 8192 (with 304ms per step).\n3.2\nSequence length bias\nIn this section, we study the effect of pretraining data sequence length on model performance in\nisolation. Using a single bucket Di as the dataset, we train an LLM from scratch on sequences with\nlength 2i for a total of 234 seen tokens. Note that the number of tokens per optimization step is fixed\nat 256, irrespective of sequence length. We use the same training hyperparameters for all runs. In\nAppendix C.2, we show that our conclusions do not depend on the choice of hyperparameters. To\n‡OpenLM-3B has 32 heads (×2 that of OpenLM-1B), and a per-head dimension of 2560/32=80, not suitable\nfor FlashAttention. This makes attention a significant bottleneck for this model.\n6\n\n\n[[PAGE 7]]\nName\nNumber of tokens ×230\nAvg.\nSeq.\nLen\nAvg.\nCtx.\nLen\nStep\nTime\n(ms)\nRegular\nMDQA\nD6\nD7\nD8\nD9\nD10\nD11\nD12\nD13\nCSR\nLU\nRC\nWK\nAvg.\n10\n20\n30\nAvg.\nNatural\n3\n6\n10\n17\n21\n17\n13\n9\n482\n1018\n244\n62.4\n65.4\n43.8\n43.9\n54.0\n26.7\n20.7\n18.5\n21.9\nEqual\n12\n12\n12\n12\n12\n12\n12\n12\n257\n1020\n244\n61.9\n64.3\n43.1\n43.5\n53.3\n25.1\n21.4\n17.4\n21.3\n1k-only\n0\n0\n0\n0\n96\n0\n0\n0\n1024\n512\n234\n60.8\n66.4\n43.2\n44.7\n54.0\n0.2\n0.1\n0.2\n0.2\n≤2k\n16\n16\n16\n16\n16\n16\n0\n0\n195\n336\n231\n62.8\n63.7\n41.8\n43.5\n53.1\n23.5\n0.4\n0.4\n8.1\n≥256\n0\n0\n16\n16\n16\n16\n16\n16\n780\n1344\n250\n61.5\n65.6\n43.4\n44.1\n53.8\n25.0\n18.4\n17.2\n20.1\nMid\n0\n0\n24\n24\n24\n24\n0\n0\n546\n480\n233\n61.9\n65.5\n42.5\n43.8\n53.6\n19.1\n0.0\n0.1\n6.4\n≥1k\n0\n0\n0\n0\n24\n24\n24\n24\n2185\n1920\n263\n61.9\n65.0\n45.8\n43.3\n54.0\n26.7\n21.6\n18.1\n22.1\nTable 1: Effect of pretraining dataset mixture on model performance. Each row corresponds to a\nmodel trained on a specific mixture of dataset decomposition buckets. All models are OpenLM-1B,\nhave seen a total of 96 × 230 tokens, use RoPE with a base frequency of 10k, and are trained with the\nsame hyperparameters. The definition of average context length is given in Appendix F.\nreduce statistical error, we train each model twice from scratch with different random seeds and\nreport the average metric for each benchmark (observing an average standard deviation of ∼0.3 for\nregular benchmarks and ∼1.6 for multi-document QA). Results are demonstrated in Fig. 5a.\nWe show a significant correlation between pretraining sequence length and different benchmarks.\nSpecifically, the accuracy of commonsense reasoning, language understanding, and world knowledge\nshows an inverted U-shape behavior with respect to pretraining sequence length, while reading\ncomprehension benefits from longer sequences. This behavior can be associated with training-test\ndistribution alignment with respect to sequence length. In Fig. 5b, we show the length distribution\nfor different benchmarks where RC demonstrates a heavier tail compared to CSR, LU, and WK.\nMulti-document QA benchmarks show a vivid correlation with respect to sequence length: test\naccuracy is ≈0 unless pretraining sequence length is greater than the test context length, which is ∼\n2k, 4k, and 6k for MDQA-10, -20, and -30, respectively.\nIt could be argued that data selection based on sequence lengths could introduce bias since the content\n(or source) of the documents might change based on the sequence lengths. To better understand the\neffect of sequence length on common metrics, we created two new buckets, D13→10 and D7→10,\nfrom existing buckets D13 and D7, respectively. The bucket D13→10 contains sequences of length 210\ncreated by chunking sequences from D13 into 8 subsequences and then performing a global shuffle.\nThe bucket D7→10 also includes sequences of length 210, each formed by concatenating 8 random\nsequences from D7.\nIn Fig. 5c, we compare the regular average metric of models pretrained on these buckets; for each\nbucket, we train two models from scratch using different random seeds and report the averaged results.\nD13→10 gains 2.6 points compared to D13 while including the same content. This demonstrates the\npure effect of sequence length on model accuracy. Furthermore, training on D13→10 underperforms\nD10 by 0.9 points, even though they are of the same length, indicating that long documents (used to\nconstruct D13→10) correlate less with our benchmarks than short documents (used to construct D10).\nFinally, we show that concatenation, as opposed to chunking, does not mitigate length correlation.\nThis is evident from the fact that D7→10 scores the same as D7 and still significantly worse than D10.\nOur analysis suggests that effective base model pretraining requires a mixture of different sequence\nlengths to perform well on all benchmarks. Next, we systematically study the effect of dataset mixture\nfrom the sequence length perspective.\n3.3\nData mixture\nA key benefit of dataset decomposition is access to and control over sequence length distribution.\nWe form datasets with different mixtures of sequence lengths and explore the performance of a\nmodel trained on each mixture. Table 1 shows the results. For all experiments, the total seen tokens\nand hyperparameters are fixed, and only the distribution over sequence length is changed. First,\nwe observe that mixtures with small average context length (we provide the exact definition in\nAppendix F) perform poorly on MDQA, which requires long context understanding. For example,\nas for “1k-only”, “≤2k”, and “Mid” distributions that do not include long sequences from D12 and\nD13. Larger average context length (e.g., as in “≥1k”) also correlates positively with performance on\n7\n\n\n[[PAGE 8]]\nName\nSampling Odds\nNum.\nCycles\nRegular\nMDQA\nD8\nD9\nD10\nD11\nD12\nD13\nCSR\nLU\nRC\nWK\nAvg.\n10\n20\n30\nAvg.\nUniform\n1\n1\n1\n1\n1\n1\n1\n62.2\n65.2\n43.4\n44.0\n53.8\n27.3\n22.0\n19.6\n23.0\nGrow-Linear\n6\n5\n4\n3\n2\n1\n1\n60.9\n64.2\n46.6\n42.9\n53.6\n30.9\n26.0\n23.9\n26.9\n8\n62.7\n65.0\n45.4\n44.7\n54.5\n30.1\n25.3\n22.8\n26.1\nGrow-P2\n32\n16\n8\n4\n2\n1\n1\n60.9\n64.3\n46.5\n44.1\n54.0\n29.6\n25.0\n23.1\n25.9\n8\n62.8\n65.2\n45.3\n44.2\n54.4\n32.3\n26.9\n24.6\n28.0\nGrow-P100\n1005\n1004\n1003\n1002\n100\n1\n1\n60.8\n65.0\n47.3\n43.4\n54.1\n30.6\n26.9\n23.5\n27.0\n8\n63.2\n65.4\n46.3\n44.6\n54.9\n30.2\n23.2\n18.9\n24.1\nShrink-P100\n1\n100\n1002\n1003\n1004\n1005\n1\n60.0\n62.2\n37.6\n40.7\n50.3\n24.5\n18.7\n15.6\n19.6\nTable 2: Effect of length-based curriculum. All models are OpenLM-1B and have seen a total of\n96 × 230 tokens, with exactly 234 tokens from each Di for i = 8, . . . , 13. We use RoPE with a base\nfrequency of 100k and the same default hyperparameters.\nreading comprehension tasks, consistent with our observation in Fig. 5a, but comes at the cost of a\nlonger training step time.\nFurthermore, “1k-only”, that is training using only the best sequence length (= 1024) from the\nstudy in Section 3.2 results in good performance on regular evaluations, especially for language\nunderstanding and world knowledge tasks, but is poor for long context tasks. Finally, we observe\nthat “natural” mixture, that is aligned with the distribution resulting from dataset decomposition (see\nFig. 3b), obtains near-optimal performance on both regular and MDQA tasks, demonstrating the\nscalability of the proposed approach to large datasets without a need for intervention on the natural\nunderlying length distribution.\n3.4\nLength-based curriculum\nWe can think of short sequences as being \"easier\" compared to longer ones; hence motivating a\ncurriculum learning  that prioritizes short sequences. A similar idea (training with image\nresolutions from low to high) is explored in vision to train CLIP  models more efficiently . In\nVSL, we can easily implement curriculum learning through sampling designs. At every optimization\nstep, we sample without replacement a batch with b tokens from bucket Di with probability pi. If a\nbucket is empty, we exclude it from sampling. We study different curricula for the \"≥256\" mixture\n(with an equal number of tokens in D8, . . . , D13). Results are shown in Table 2. For each curriculum,\nwe determine the odds of picking a batch from each bucket (= pi’s when normalized). Details of our\nlength-based sampling and curriculum are provided in Algorithm 1. We consider curricula that shift\nfrom short to long sequences at different paces controlled by pi’s changing linearly, with powers of 2,\nand with powers of 100 between buckets.\nDue to the presence of other hyperparameter schedules during the course of training (e.g., learning\nrate and weight decay), a curriculum on length may result in a potential implicit bias. For example,\nif we only see long sequences toward the end of training, long sequence learning occurs only when\nthe learning rate is too small. To address this potential issue, we also explore cyclic curricula, where\na curriculum is applied in cycles similar to cyclic learning rate schedules  as shown in Fig. 6.\nNote that when we train on a sequence of length l, we have l next-token prediction losses (applied in\nparallel) with context lengths 0, 1, . . . , l −1. This already implies some mixing: when training on a\n“hard” example (i.e., a long sequence), we also include “easy” examples (its shorter sub-sequences).\nTherefore, even towards the end of each cycle, we still have some losses with short contexts.\nOur results show that the cyclic \"Grow-P2\" curriculum is near optimal with different metrics. An\nadditional benefit of curriculum is training stability. Li et al.  noticed that long sequences\ncontribute to extreme gradient variance, especially at the beginning of training, resulting in instability.\nWe also observe (see Appendix E) that our proposed approach with curriculum results in more stable\ntraining dynamics, thus enabling more efficient training with larger batch sizes and learning rates.\n3.5\nScaling\nDataset scaling\nIn Fig. 1a, we show the performance of models trained with 234, 235, 236, 237, and\n238 total tokens using DD and baseline. We use the “≥256” mixture and “Grow-Linear” curriculum\nwith 8 cycles for DD, and a fixed target sequence length 8192 for the baseline. Results show > 2×\ndata efficiency: our proposed method reaches the same accuracy as the baseline using less than half\nthe tokens.\n8\n\n\n[[PAGE 9]]\nModel\nSize\n\nNum\nTime\n∆\nRegular\n∆\nMDQA\n∆\nGPUs\n(hours)\nAvg.\nAvg.\n160M\nBaseline-8k\n16\n18.3\n39.3\n9.7\nDD\n15.7\n-14%\n40.0\n+0.7\n11.4\n+1.7\n410M\nBaseline-8k\n16\n38.9\n48.3\n14.8\nDD\n29.6\n-24%\n49.4\n+1.1\n18.8\n+4.0\n1B\nBaseline-8k\n32\n44.4\n56.7\n25.6\nDD\n35.4\n-20%\n58.4\n+1.7\n25.6\nTable 3: Comparing baseline training with DD on an alternative pretraining dataset and model sizes.\n\nfb\nRegular\nMDQA\nAvg.\nAvg.\nBaseline-8k\n10k\n51.3\n19.0\n100k\n51.5\n24.4\nDD≥256\n10k\n53.8\n20.1\n100k\n53.8\n24.9\nTable 4: Effect of RoPE base frequency, fb, in pretraining.\nModel scaling\nWe report results on OpenLM-1B, -3B, and -7B trained from scratch for a total of\n237 tokens in Fig. 4b. We compare baseline training with a fixed target sequence length 8192 and\nVSL training with a DD≥256 mixture and the \"Grow-Linear\" curriculum with 8 cycles. Training with\nDD results in significant accuracy gains and reductions in training wall-clock time at different scales.\nAlternative dataset\nWe demonstrate the efficacy of our proposed method on another large-scale\ndataset, DataComp-LM . We train models with different numbers of parameters: OpenLM-160M,\n-410M, and -1B, for a total of 137B tokens. We compare the baseline with a DD≥256 mixture\ntrained with the \"Grow-P2\" curriculum with 8 cycles. Results are reported in Table 3, demonstrating\nsignificant accuracy and training efficiency gains.\n3.6\nComparison with state-of-the-art\nWe compare our proposed method, data decomposition, with other approaches for handling various\ndocument lengths of pretraining data, including document masking (DM), best-fit sequence packing , and in-context pretraining (ICLM) . We describe the details of our implementation of\nthe best-fit packing in Appendix D. For ICLM, we use the official implementation§ applied to the\nRefinedWeb dataset. The results are shown in Table 5.\nPre-training context length is an important factor in determining a model’s long-context performance.\nWe empirically validate this in the results shown in Fig. 5a, where models trained on longer sequences\nperform better on multi-document QA. Our proposed method has an average context length (as\ndefined in Eq. (2)) of 1,344 for the RefinedWeb dataset, compared to 930 for the baseline (see Fig. 3c)\nand 1,064 when packing  is applied. This explains why the dataset decomposition mixture, even\nwithout any length-based curriculum (the first row in Table 2), outperforms Baseline-8k-DM and\nPack-8k+DM (second and third rows in Table 5). Here, DM refers to applying document masking\nduring training to avoid cross-document attention.\nDocument masking improves the baseline on regular evaluations from 51.5 to 52.4 by preventing\ncross-document attention. However, Xiong et al.  demonstrate that including concatenated\nunrelated documents can still enhance long-context metrics compared to training solely with shorter\nsequences. Therefore, DM experiences a slight decline in long-context evaluations, dropping from\n27.5 to 27.1. Baseline-8k multi-document QA performance is even slightly better than our proposed\ndataset decomposition mixture when used without length-based curriculum (the first row in Table 2).\nIn-context pre-training LMs (ICLM)  proposes document sorting based on content similarity.\nAlthough the benefits of ICLM with large-scale Common Crawl data (used in our experiments) are\nmarginal in regular evaluation, we observe that ICLM results in slightly better multi-document QA\nperformance than Baseline-8k when 30 documents are in the context compared with Baseline-8k\n(22.0% vs. 20.5%). The average long-context metric boosts from 27.5 for Baseline-8k to 28.7 for\nICLM. However, the similarity finding step proposed by ICLM is resource-intensive at scale¶.\nFinally, as shown in in Table 2 our proposed cyclic length-based curriculum, for example, Grow-P2\nwith 8 cycles, results in a significant improvement in the model’s long-context capability. Our\nproposed method avoids cross-document attention to unrelated content, maintains coherent long\nsequences, and benefits from a length-based curriculum, effectively improving performance in both\nregular and long-context evaluations compared to all baselines. We further summarize long-context\nperformance of different methods discussed above in Table 6.\n§https://github.com/swj0419/in-context-pretraining\n¶Processing 400B tokens with the official repository required over a week using 96 CPUs and 16 GPUs.\n9\n\n\n[[PAGE 10]]\nRegular\nLong Context\nStep\nTime\n(ms)\nData\nPrep.\nCost\n\nCSR\nLU\nRC\nWK\nAvg.\nMDQA\nTOEFL\nQuALITY\nAvg.\n10\n20\n30\nBaseline-8k\n60.6\n62.5\n41.5\n41.3\n51.5\n29.0\n23.8\n20.5\n26.2\n32.0\n27.5\n304\n$\nBaseline-8k+DM\n60.2\n64.1\n42.8\n41.8\n52.4\n24.4\n20.0\n16.0\n29.2\n32.0\n27.1\n304\n$\nPack-8k+DM\n60.3\n64.0\n44.6\n41.8\n52.7\n25.6\n19.8\n16.9\n29.2\n33.1\n27.7\n304\n$$\nICLM\n60.6\n62.1\n44.7\n40.0\n51.7\n26.7\n20.0\n22.0\n28.7\n34.6\n28.7\n304\n$$$\nDD (ours)\n62.8\n65.2\n45.3\n44.2\n54.4\n32.3\n26.9\n24.6\n30.7\n34.2\n30.9\n244\n$\nTable 5: Comparison with baseline and state-of-the-art methods. All models are trained with the same\nhyperparameters, RoPE with fb = 100k, and for 103B tokens. DM denotes training with document\nmasking. DD uses the \"Grow-P2\" curriculum with 8 cycles. Dataset preparation cost is symbolic to\ncompare methods and does not reflect the wall-clock time.\n4\nRelated works\n\nDoc\nMasking\nAverage\nContext\nDocs in a\nSequence\nCurr.\nMDQA30 (%)\nBaseline\n✓\n930\nMult-random\n✗\n16.0\nPack8k+DM\n✓\n1064\nMult-packing\n✗\n16.9\nDD-Uniform\n✗\n1344\nSingle\n✗\n19.6\nBaseline\n✗\n4096\nMult-random\n✗\n20.5\nICLM\n✗\n4096\nMult-semantic\n✗\n22.0\nDD-Grow-P2\nN/A\n1344\nSingle\n✓\n24.6\nTable 6: Summary of long-context performance\nfor different methods from Table 2 and Table 5.\nRecent works have raised concerns regarding\ncross-document attention. For example, Llama3 , ICLM , and , which we discussed in Section 3.6.\nSimilarly,  discuss challenges with the baseline concat-andchunk approach and propose an approximate\nbin-packing algorithm.\nRelated to our study on sequence length bias,\n shows the importance of train-vs-test time\ndistribution shift from a sequence length perspective on a string editing task.\nhighlight the challenge of generalizing to lengths beyond what the model has seen during training\nand discuss the importance of positional encoding. Several works\naddress enabling LLM inference with long context (see  for an overview). These approaches\nare orthogonal to our contribution and can be applied post-pretraining to adapt to longer lengths.\nGrowLength  proposes accelerating LLM pretraining by progressively growing context length using the baseline sequence formation method, but does not show results on LLMs. Similarly, increasing\nsequence length has been shown in BERT model training  to improve compute efficiency.\nThe idea of dynamic batching has been explored in other domains. In vision, methods like NaViT  use images with variable resolutions (a similar concept to context length for LLMs). In seq-to-seq\ntasks (e.g., automatic speech recognition, text-to-speech, and neural machine translation), the inputs\nhave different lengths. An efficient approach is to sort inputs by their length and form batches of\ninputs with similar lengths during training (after possible padding). Batchsize is dynamically adjusted\ninversely proportional to input lengths . Different from these works, in dataset decomposition,\nwe do not simply put documents with similar lengths into the same bucket. Instead, we decompose\neach document into multiple subsequences and form multiple buckets. We form batches with different\nlengths during training by sampling from these buckets using a target mixture and curriculum.\n5\nConclusion and limitations\nIn this paper, we explore the shortcomings of a popular LLM pretraining approach, concat-andchunk, and introduce dataset decomposition, a method to decompose a dataset of text documents into\nbuckets containing fixed sequence lengths. We show results of variable sequence training using DD\nwith different mixtures, curricula, datasets, and models, demonstrating significant LLM pretraining\nspeedup and a final model accuracy boost on a wide range of benchmarks. Furthermore, we provide\nanalysis on sequence length bias and attention masking. We compare our proposed method with\nrecent works that also address concat-and-chunk shortcomings in a unified experimental setup and\nshow gains in data preparation cost, training time, and final model accuracy.\nLimitations. The training speed gains compared to the baseline are significant only when the target\nsequence length is long enough. Otherwise, the attention cost is not a dominant fraction of training,\nand hence no significant training speedup is expected.\n10\n\n\n[[PAGE 11]]\n\n\n\n[[PAGE 12]]\nJ. L. Elman. Learning and development in neural networks: The importance of starting small.\nCognition, 48(1):71-99, 1993.\n L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,\nN. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv\npreprint arXiv:2101.00027, 2020.\n Z. Ge, L. Kaushik, M. Omote, and S. Kumar. Speed up training with variable length inputs by\nefficient batching strategies. In Interspeech, pages 156-160, 2021.\n P. Gonzalez, T. S. Alstrøm, and T. May. On batching variable size inputs for training end-toend speech enhancement systems. In ICASSP 2023-2023 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023.\n S. Gururangan, M. Wortsman, S. Y. Gadre, A. Dave, M. Kilian, W. Shi, J. Mercat, G. Smyrnis, G. Ilharco, M. Jordan, R. Heckel, A. Dimakis, A. Farhadi, V. Shankar, and L. Schmidt.\nOpenLM: a minimal but performative language modeling (lm) repository, 2023.\nURL\nhttps://github.com/mlfoundations/open_lm/. GitHub repository.\n C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, and S. Wang. Lm-infinite: Simple on-the-fly length\ngeneralization for large language models. arXiv preprint arXiv:2308.16137, 2023.\n H. Jin, X. Han, J. Yang, Z. Jiang, C.-Y. Chang, and X. Hu. Growlength: Accelerating llms\npretraining by progressively growing training length. arXiv preprint arXiv:2310.00576, 2023.\n A. Kazemnejad, I. Padhi, K. Natesan Ramamurthy, P. Das, and S. Reddy. The impact of\npositional encoding on length generalization in transformers. Advances in Neural Information\nProcessing Systems, 36, 2024.\n M. M. Krell, M. Kosec, S. P. Perez, and A. Fitzgibbon. Efficient sequence packing without\ncross-contamination: Accelerating large language models without impacting performance.\narXiv preprint arXiv:2107.02027, 2021.\n T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein,\nI. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering\nresearch. Transactions of the Association for Computational Linguistics, 7:453-466, 2019.\n K. Lee, M.-W. Chang, and K. Toutanova. Latent retrieval for weakly supervised open domain\nquestion answering. arXiv preprint arXiv:1906.00300, 2019.\n B. Lefaudeux, F. Massa, D. Liskovich, W. Xiong, V. Caggiano, S. Naren, M. Xu, J. Hu,\nM. Tintore, S. Zhang, P. Labatut, D. Haziza, L. Wehrstedt, J. Reizenstein, and G. Sizov.\nxformers: A modular and hackable transformer modelling library. https://github.com/\nfacebookresearch/xformers, 2022.\n H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In Thirteenth\ninternational conference on the principles of knowledge representation and reasoning, 2012.\n C. Li, M. Zhang, and Y. He. The stability-efficiency dilemma: Investigating sequence length\nwarmup for training gpt models. Advances in Neural Information Processing Systems, 35:\n26736-26750, 2022.\n J. Li, A. Fang, G. Smyrnis, M. Ivgi, M. Jordan, S. Gadre, H. Bansal, E. Guha, S. Keh, K. Arora,\net al. Datacomp-lm: In search of the next generation of training sets for language models. arXiv\npreprint arXiv:2406.11794, 2024.\n X. Li, Z. Wang, and C. Xie. An inverse scaling law for clip training. Advances in Neural\nInformation Processing Systems, 36, 2024.\n H. Liu, W. Yan, M. Zaharia, and P. Abbeel. World model on million-length video and language\nwith blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024.\n N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang. Lost in\nthe middle: How language models use long contexts. Transactions of the Association for\nComputational Linguistics, 12:157-173, 2024.\n12\n\n\n[[PAGE 13]]\nX. Liu, H. Yan, S. Zhang, C. An, X. Qiu, and D. Lin. Scaling laws of rope-based extrapolation.\narXiv preprint arXiv:2310.05209, 2023.\n Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\nand V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n S. Mehta, F. Abdolhosseini, and M. Rastegari. Cvnets: High performance library for computer\nvision. In Proceedings of the 30th ACM International Conference on Multimedia, pages\n7327-7330, 2022.\n Meta. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL\nhttps://ai.meta.com/blog/meta-llama-3.\n T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can a suit of armor conduct electricity? a\nnew dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\n A. Mohtashami and M. Jaggi. Random-access infinite context length for transformers. Advances\nin Neural Information Processing Systems, 36, 2024.\n K. Nagatsuka, C. Broni-Bediako, and M. Atsumi. Pre-training a bert with curriculum learning\nby increasing block-size of input text. In Proceedings of the International Conference on Recent\nAdvances in Natural Language Processing (RANLP 2021), pages 989-996, 2021.\n M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. fairseq: A fast,\nextensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations,\n2019.\n D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni,\nG. Boleda, and R. Fernández. The lambada dataset: Word prediction requiring a broad discourse\ncontext. arXiv preprint arXiv:1606.06031, 2016.\n S. Pawar, S. Tonmoy, S. Zaman, V. Jain, A. Chadha, and A. Das. The what, why, and how of\ncontext length extension techniques in large language models-a detailed survey. arXiv preprint\narXiv:2401.07872, 2024.\n G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier,\nE. Almazrouei, and J. Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated\ncorpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. URL\nhttps://arxiv.org/abs/2306.01116.\n B. Peng and J. Quesnelle.\nNtk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.\n2023.\nURL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/\nntkaware_scaled_rope_allows_llama_models_to_have.\n B. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of\nlarge language models. arXiv preprint arXiv:2309.00071, 2023.\n A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\nIn International conference on machine learning, pages 8748-8763. PMLR, 2021.\n P. Rajpurkar, R. Jia, and P. Liang. Know what you don’t know: Unanswerable questions for\nsquad. arXiv preprint arXiv:1806.03822, 2018.\n S. Reddy, D. Chen, and C. D. Manning. Coqa: A conversational question answering challenge.\nTransactions of the Association for Computational Linguistics, 7:249-266, 2019.\n M. Roemmele, C. A. Bejan, and A. S. Gordon. Choice of plausible alternatives: An evaluation\nof commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.\n A. Ruoss, G. Delétang, T. Genewein, J. Grau-Moya, R. Csordás, M. Bennani, S. Legg, and\nJ. Veness. Randomized positional encodings boost length generalization of transformers. arXiv\npreprint arXiv:2305.16843, 2023.\n13\n\n\n[[PAGE 14]]\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd\nschema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.\n W. Shi, S. Min, M. Lomeli, C. Zhou, M. Li, V. Lin, N. A. Smith, L. Zettlemoyer, S. Yih, and\nM. Lewis. In-context pretraining: Language modeling beyond document boundaries. arXiv\npreprint arXiv:2310.10638, 2023.\n L. N. Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter\nconference on applications of computer vision (WACV), pages 464-472. IEEE, 2017.\n L. Soldaini, R. Kinney, A. Bhagia, D. Schwenk, D. Atkinson, R. Authur, B. Bogin, K. Chandu,\nJ. Dumas, Y. Elazar, V. Hofmann, A. H. Jha, S. Kumar, L. Lucy, X. Lyu, N. Lambert, I. Magnusson, J. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, A. Ravichander, K. Richardson, Z. Shen, E. Strubell, N. Subramani, O. Tafjord, P. Walsh, L. Zettlemoyer, N. A. Smith,\nH. Hajishirzi, I. Beltagy, D. Groeneveld, J. Dodge, and K. Lo. Dolma: an Open Corpus of Three\nTrillion Tokens for Language Model Pretraining Research. arXiv preprint, 2024.\n J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with\nrotary position embedding. Neurocomputing, 568:127063, 2024.\n H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023.\n H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023.\n B.-H. Tseng, S.-S. Shen, H.-Y. Lee, and L.-S. Lee. Towards machine comprehension of spoken\ncontent: Initial toefl listening comprehension test by machine, 2016.\n D. Variš and O. Bojar. Sequence length is a domain: Length-based overfitting in transformer\nmodels. arXiv preprint arXiv:2109.07276, 2021.\n A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and\nI. Polosukhin. Attention is all you need. Advances in neural information processing systems,\n30, 2017.\n W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A.\nSankararaman, B. Oguz, et al. Effective long-context scaling of foundation models. arXiv\npreprint arXiv:2309.16039, 2023.\n R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really\nfinish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n Y. Zhou, U. Alon, X. Chen, X. Wang, R. Agarwal, and D. Zhou. Transformers can achieve\nlength generalization but not robustly. arXiv preprint arXiv:2402.09371, 2024.\n D. Zhu, N. Yang, L. Wang, Y. Song, W. Wu, F. Wei, and S. Li. Pose: Efficient context window\nextension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023.\n14\n\n\n[[PAGE 15]]\nA\nBroader impacts\nThis work enables faster training of LLMs, which are among the most compute-intensive applications\nin the field. A positive societal/environmental impact of this work is training LLMs with a smaller\ncarbon footprint.\nAnother potential societal advantage of this work is training LLMs with fewer hallucinations. While\nwe did not directly measure this potential benefit, a concurrent work  shows such a benefit when\ncross-document attention is not allowed during LLM pretraining.\nB\nImplementation details\nB.1\nTraining details\nSoftware and hardware details\nAll experiments in this paper are conducted using the OpenLM||\nrepository, which is based on PyTorch. We use Fully Sharded Data Parallelism (FSDP) with Bfloat16\nmixed precision for all experiments. We use the Xformers  implementation for attention. For\nhardware, we use one or more nodes of 8× NVIDIA H100 GPUs (Hopper architecture), each with\n80GB memory, and 192 CPU cores with 2000GB of RAM. Nodes are connected through Elastic\nFabric Adapter (EFA) for efficient inter-node communication hosted by AWS.\nModel architecture details\nWe provide details of all architectures used in the paper in Table 7 to\nTable 11.\nModel Name\nOpenLM-160M\nHidden dimension\n768\nNumber of Layers\n12\nNumber of Heads\n12\nNumber of Parameters\n162,435,840\nTable 7: OpenLM-160M.\nModel Name\nOpenLM-410M\nHidden dimension\n1024\nNumber of Layers\n24\nNumber of Heads\n16\nNumber of Parameters\n411,665,408\nTable 8: OpenLM-410M.\nModel Name\nOpenLM-1B\nHidden dimension\n2048\nNumber of Layers\n24\nNumber of Heads\n16\nNumber of Parameters\n1,439,893,504\nTable 9: OpenLM-1B.\nModel Name\nOpenLM-3B\nHidden dimension\n2560\nNumber of Layers\n32\nNumber of Heads\n32\nNumber of Parameters\n2,796,096,000\nTable 10: OpenLM-3B.\nModel Name\nOpenLM-7B\nHidden dimension\n4096\nNumber of Layers\n32\nNumber of Heads\n32\nNumber of Parameters\n6,889,672,704\nTable 11: OpenLM-7B.\nBaseline hyper parameters\nWe list our baseline hyperparameters in Table 12 and iterate over\nchanges for each section next. Note that we did not explicitly optimize hyperparameters for any of\nthe experiments, and we always use the same hyperparameters when using either the baseline method\nor ours.\n||https://github.com/mlfoundations/open_lm\n15\n\n\n[[PAGE 16]]\nOptimizer\nAdamW\nAdamW-β1\n0.9\nAdamW-β2\n0.95\nlearning-rate schedule\ncosine+warmup\nMaximum learning rate\n3 × 10−3\ncooldown learning rate\n3 × 10−5\nWarm-up steps\n5000\nGrad Norm Clipping\n1.0\nGlobal batchsize (num tokens per step)\n219\nWeight Decay\n0.1\nZ-Loss Coefficient\n10−4\nTable 12: Baseline hyper-parameters.\nImplementation details of Section 3.2 experiments\nExperiments in this section are done using\nthe same hyperparameters as in Table 12 for a total of 234 tokens on the OpenLM-1B model. We\ntrained each model twice with different random seeds and report the averaged results. For models in\nthis section, we use RoPE with fb = 10, 000. In Table 12, we show that our results and conclusions\nin this section are not sensitive to hyperparameters, including the RoPE base frequency fb.\nImplementation details of Section 3.3 and Section 3.4 experiments\nExperiments in this section\nare done with OpenLM-1B model, trained for total of 96 × 1034 ≈103B tokens. Hyper-parameters\nare the same as Table 12, except we used 20000 warmup steps for all models presented in this section.\nWe use RoPE with fb = 10, 000 for all models in Section 3.3 and fb = 100, 000 for models in\nSection 3.4.\nImplementation details of Section 3.5\nDataset scaling: Experiments in this section are trained with the OpenLM-1B model, RoPE with\nfb = 100, 000, and the baseline setup as in Table 12 except for the following changes for different\ndataset sizes:\n• total tokens = 234, warmup steps = 5, 000\n• total tokens = 235, warmup steps = 5, 000\n• total tokens = 236, warmup steps = 10, 000\n• total tokens = 237, warmup steps = 20, 000\n• total tokens = 238, warmup steps = 40, 000\nModel scaling: Experiments in this section are trained with the OpenLM-1B, OpenLM-3B, and\nOpenLM-7B models, 237 ≈137B total seen tokens, RoPE with fb = 100, 000, and the baseline\nsetup as in Table 12 except for the following changes for different model sizes:\n• OpenLM-1B, warmup steps = 20, 000, max-lr = 3 × 10−3, batchsize b = 219, with 32\nH100 GPUs\n• OpenLM-3B, warmup steps = 20, 000, max-lr = 2 × 10−3, batchsize b = 220, with 64\nH100 GPUs\n• OpenLM-7B, warmup steps = 20, 000, max-lr = 1 × 10−3, batchsize b = 222, with 128\nH100 GPUs\nAlternative dataset: Experiments in this section are trained with the OpenLM-160M, OpenLM410M, and OpenLM-1B models, 237 ≈137B total seen tokens, RoPE with fb = 100, 000, and the\nbaseline setup as in Table 12 except for the following changes for different model sizes:\n• OpenLM-160M, warmup steps = 20, 000, max-lr = 5 × 10−3, weight-decay = 0.033, with\n16 H100 GPUs\n• OpenLM-410M, warmup steps = 20, 000, max-lr = 4 × 10−3, weight-decay = 0.066, with\n16 H100 GPUs\n• OpenLM-1B, warmup steps = 20, 000, max-lr = 3 × 10−3, weight-decay = 0.1, with 32\nH100 GPUs\nFor the DD experiments we used “Grow-P2” length curriculum which is visualized in Fig. 6.\n16\n\n\n[[PAGE 17]]\nFigure 6: Comparison of length-based curriculum schedule with learning rate schedule. Sequence\nlength varies between 256 and 8192 based on the Grow-P2 curriculum with 8 cycles. Note that the\nchoice of bucket (and hence the sequence length) is random, with sampling probabilities determined\nby the curriculum. In the figure, we show the length of the sampled sequence at every 9 optimization\nsteps. For the learning rate, we use a cosine learning rate with a warm-up for 4k steps. The job\ncorresponds to training for a total of 236 tokens, with 220 tokens seen per optimization step.\nImplementation details of Section 3.6\nAll experiments in this section are done with the OpenLM1B model, trained for a total of 96 × 1034 ≈103B tokens. Hyperparameters are the same as in\nTable 12, except we used 20,000 warmup steps for all models presented in this section. We use RoPE\nwith fb = 100, 000.\nB.2\nLength based sampling and curriculum algorithm\nWe present the details of our length-based sampling and curriculum in Algorithm 1.\nAlgorithm 1 Length based sampling and curriculum\nRequire:\n• Di: list of buckets such that Di includes sequences with length 2i\n• ni: total number of tokens to be picked from each bucket (see Table 1)\n• oi: sampling odd for each bucket (see Table 2)\n• c: number of cycles\n• b: number of tokens per optimization step\nsi,j ←random subset of Di with ni/c tokens\n▷non-overlapping subsets of Di\nfor j ∈[1, 2, . . . , c] do\n▷loop over cycles\nwhile at least one si,j is non-empty do\nodds ←[oi if si,j is not empty else 0 for i = 1, 2, 3, . . .]\nprobs ←odds/odds.sum()\nrandomly sample index i with probability probs[i]\nsample b/2i sequences from si,j w/o replacement for training\nend while\nend for\nB.3\nEvaluation details\nMulti Document Question Answering (MDQA)\nWe follow the open-book evaluation setup\ndescribed in . The document containing the answer is part of the context. The evaluation script\nprovided by the official repository processes the model’s response by using only the text before the\nfirst occurrence of a newline character as the answer. We noticed that sometimes the model responds\nwith multiple newline characters before providing any valid text. In view of this behavior, we updated\nthe evaluation script to look for the first non-empty text output from the model instead of the first\nstring after newline character. Apart from this change in processing the model output, the rest of the\nevaluation follows the official implementation .\nTOEFL\nWe follow the setup described in . As described in Section 3, the dataset contains\nmultiple-choice QA pairs for the 15 longest lectures in . To obtain a response from the\nmodel, we follow MMLU-style prompting, where the choices are appended to the original prompt\nindividually and the mean log-probability is computed for each choice. The choice corresponding to\n17\n\n\n[[PAGE 18]]\n64\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\nsequence length\n1\n2\n4\n8\n16\n32\n64\n128\n256\nnumber of sequences per batch\n100\n99\n100\n100\n100\n107\n153\n304\n719\n99\n100\n100\n101\n106\n143\n265\n566\n100\n102\n100\n105\n138\n243\n487\n100\n100\n106\n137\n232\n445\n102\n105\n135\n227\n426\n105\n135\n225\n416\n136\n224\n411\n224\n409\n410\n(a) Average time for OpenLM-1B\n64\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\nsequence length\n1\n2\n4\n8\n16\n32\n64\n128\n256\nnumber of sequences per batch\n144\n143\n143\n144\n154\n203\n344\n777\n2218\n143\n143\n144\n151\n190\n292\n589\n1452\n143\n144\n152\n185\n267\n501\n1095\n144\n150\n180\n254\n449\n924\n149\n180\n245\n428\n832\n180\n242\n411\n789\n246\n406\n758\n412\n751\n760\n(b) Average time for OpenLM-3B\n64\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\nsequence length\n1\n2\n4\n8\n16\n32\n64\n128\n256\nnumber of sequences per batch\n269\n270\n269\n273\n286\n348\n562\n1091\n269\n268\n274\n285\n344\n538\n993\n269\n274\n285\n341\n526\n938\n273\n284\n340\n520\n911\n285\n340\n516\n899\n340\n515\n891\n515\n888\n889\n(c) Average time for OpenLM-7B\n64\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\nsequence length\n1\n2\n4\n8\n16\n32\n64\n128\n256\nnumber of sequences per batch\n0.53\n0.22\n0.18\n0.17\n0.52\n0.09\n0.23\n2.02\n2.60\n0.13\n0.25\n0.45\n0.45\n0.10\n0.36\n0.77\n0.62\n0.27\n1.05\n0.24\n0.19\n0.25\n0.67\n1.00\n0.10\n0.18\n0.38\n0.04\n0.50\n0.63\n0.38\n0.31\n0.15\n0.59\n0.72\n0.42\n0.25\n0.46\n0.62\n0.16\n0.59\n0.54\n0.48\n0.55\n0.39\n(d) STD for OpenLM-1B\n64\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\nsequence length\n1\n2\n4\n8\n16\n32\n64\n128\n256\nnumber of sequences per batch\n0.62\n0.44\n0.42\n0.29\n0.54\n0.34\n0.59\n4.58\n4.58\n0.35\n0.28\n0.26\n0.23\n0.63\n0.21\n1.11\n3.34\n0.48\n0.40\n0.36\n0.40\n0.28\n0.65\n0.94\n0.29\n0.49\n0.33\n0.44\n0.92\n0.68\n0.34\n0.34\n0.39\n0.60\n0.76\n0.53\n0.61\n1.04\n0.56\n0.45\n0.82\n1.17\n0.82\n0.86\n0.79\n(e) STD for OpenLM-3B\n64\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\nsequence length\n1\n2\n4\n8\n16\n32\n64\n128\n256\nnumber of sequences per batch\n0.47\n0.64\n0.40\n0.40\n0.41\n1.10\n2.25\n1.89\n0.28\n0.35\n0.32\n0.38\n0.39\n0.64\n1.20\n0.58\n0.31\n0.30\n0.24\n0.37\n0.63\n0.31\n0.41\n0.32\n0.26\n0.79\n0.39\n0.32\n0.33\n0.35\n0.34\n0.67\n0.37\n0.69\n0.65\n0.87\n(f) STD for OpenLM-7B\nFigure 7: Top row: Average time (ms) for each node to train one batch on a 8×H100 machine using\nFSDP. Bottom row: measured standard deviation for each setup.\nthe argmax of mean log-probability is then chosen as the model’s response. After we obtain the\nresponse, the computation of accuracy follows the official implementation .\nQuALITY\nWe follow the setup described in . The dataset contains long documents with each\ndocument containing multiple-choice QA pairs. Sometimes the context for a QA pair can be longer\nthan 8192 tokens. To account for the longer sequence length, we increase the base frequency of\nRoPE positional encoding from 100k to 200k without any fine-tuning. To obtain a response from the\nmodel, we follow MMLU-style prompting, where the choices are appended to the original prompt\nindividually and the mean log-probability is computed for each choice. The choice corresponding to\nthe argmax of mean log-probability is then chosen as the model’s response. After we obtain the\nmodel output, the rest of the evaluation follows the official implementation .\nC\nAdditional results\nC.1\nAdditional results for training efficiency\nWe enumerate model sizes (OpenLM-1B, OpenLM-3B, OpenLM-7B), the number of sequences in a\nbatch (from 1 to 256), and sequence lengths (26 to 214) and measure the time to train 100 batches. We\nrepeat this 5 times and report the average and standard deviation time per batch in Fig. 7. Notice that\nin the figure, each diagonal corresponds to a fixed b (number of tokens seen per optimization step).\nC.2\nAdditional results for sequence length bias experiments\nIn this section, we show that changing hyperparameters does not alter our conclusions in Section 3.2.\nWe observed that pretraining on a sequence length of 1024 results in optimal performance with\nrespect to regular metrics, compared to both longer and shorter lengths. For example, the regular\naverage metric is 48.0 when pretraining with a 1024 sequence length, but it is 47.0 when pretraining\nwith a 2048 sequence length. We explore whether this gap can be filled by using potentially\nbetter hyperparameters when training with a 2048 sequence length. Results are shown in Table 13,\n18\n\n\n[[PAGE 19]]\ndemonstrating that the gap cannot be simply filled by choosing a different hyperparameter and is\nfundamental to the choice of pretraining sequence length.\nMaximum Learning Rate\nRoPE fb\nRegular Average\n3 × 10−3\n10,000\n47.0\n3 × 10−3\n100,000\n47.1\n10−3\n10,000\n45.9\n10−2\n10,000\n46.5\nTable 13: Sensitivity to hyperparameters for Section 3.2 experiments. All models are trained twice\nwith different random seeds, and averaged results\nare reported.\nC.3\nAdditional results for scaling experiments\nIn this section, we show additional results for the experiments presented in Section 3.5. Table 14\nshows results for dataset scaling, Table 15 for model scaling, and Table 16 for experiments on an\nalternative dataset.\nSeen tokens\n\nRegular average\nMDQA average\n234\nBaseline-8k\n45.2\n7.8\nDD\n47.0\n16.0\n235\nBaseline-8k\n47.6\n15.4\nDD\n50.6\n23.3\n236\nBaseline-8k\n50.2\n19.9\nDD\n52.1\n22.3\n237\nBaseline-8k\n51.9\n23.2\nDD\n54.9\n25.9\n238\nBaseline-8k\n53.6\n25.8\nDD\n56.0\n29.4\nTable 14: Dataset scaling for OpenLM-1B.\nModel size\n\nRegular average\nMDQA average\n1B\nBaseline-8k\n51.9\n23.1\nDD\n54.9\n24.2\n3B\nBaseline-8k\n57.5\n17.8\nDD\n59.0\n31.1\n7B\nBaseline-8k\n59.8\n31.7\nDD\n62.5\n34.7\nTable 15: Model scaling for total of 137B tokens.\nSize\n\nPIQA\nCOPA\nOBQA\nLamOAI\nHelSwg\nWinG\nWinGE\nSQuaAD\nBoolQ\nCoQA\nJeop\nArcE\nArcC\nWikiQA\nMDQA\n0-shot\n0-shot\n10-shots\n0-shot\n0-shot\n3-shots\n5-shots\n3-shots\n,0-shot\n0-shot\n3-shots\n3-shots\n3-shots\n3-shots\n10\n20\n30\n160M\nBaseline\n66.5\n61\n29.2\n40.5\n37.2\n63.4\n51.9\n12.9\n55.6\n18.2\n2.3\n49.5\n25.9\n36.2\n12.8\n9.3\n7.1\nDD\n66.4\n66\n30.2\n43.6\n37.7\n66.3\n52.2\n14.3\n50.7\n19.1\n4.3\n51.5\n24\n34\n16.2\n9.9\n8.2\n410M\nBaseline\n69.8\n68\n37.4\n53.0\n50.4\n74.0\n55.8\n30.0\n59.7\n28.5\n12.1\n59\n29.8\n48.3\n18.9\n13.4\n12\nDD\n71.5\n70\n38\n55.8\n51.6\n74.7\n56.3\n27\n59.5\n26.2\n17.6\n60.4\n30.5\n52.2\n24.4\n18.1\n14\n1B\nBaseline\n74.9\n74\n43.4\n63\n62.7\n80.2\n63.4\n41.8\n64.1\n35.3\n29.7\n65.7\n38.4\n56.7\n31.3\n24.8\n20.8\nDD\n76.7\n75\n42.6\n64.7\n64.7\n82.8\n65\n41.8\n66.4\n38.3\n32.6\n68.4\n39.8\n58.7\n31.6\n24.7\n20.4\nTable 16: Small model performance trained on an improved refined-web pipeline applied to Common\nCrawl. All models are trained for a total of 237 tokens.\nD\nComparison to best-fit sequence packing\nSome recent works have employed a bin packing-based strategy  which aims to reduce document\ncross-attention by minimizing unnecessary document truncation. To achieve this, they implement\na known approximation algorithm called best-fit decreasing, which packs document chunks into\nsequences as tightly as possible. To compare with our method, we created a new dataset based on\nour implementation of the best-fit decreasing algorithm and trained a new model using this dataset.\nWe present our implementation of the best-fit decreasing algorithm, the dataset we created, and the\nmodel we trained for comparison.\nGiven a dataset D, the input to the algorithm is a list of tokenized document chunks C =\n{c1, c2, . . . , cK} such that SK\ni=1 ci = D, where each chunk is at most context size n (e.g., 2048) in\nlength. The output of the algorithm is a list of bins B = {b1, b2, . . . , bM} such that ci ∈bj. As a\npre-processing step, we first tokenize the documents and convert them into chunks. Truncation is\napplied during this step only when necessary. Next, we sort the chunks from largest to smallest and\nstart from the first chunk to pack into bins of size n. We track the remaining capacities for each\n19\n\n\n[[PAGE 20]]\nbin while we iterate over the chunks. In each iteration, the algorithm finds the best bin that is both\nfeasible and optimal for placing the chunk. Feasible bins are those that can accommodate the chunk,\nand optimal bins are those left with the minimum remaining capacity after placing the chunk. If\nsuch a bin is not found, we open a new bin and place the chunk inside. After all the chunks have\nbeen placed, we select the bins that have non-zero remaining capacities and fill them with pad tokens\n<PAD>.\nWe process the RefinedWeb  dataset using the aforementioned procedure and create training\nsequences by concatenating all chunks in a bin. Figure 3 shows that while best-fit packing results in\na higher average context length compared to the baseline concat-and-chunk, it is still much lower\ncompared to our method dataset decomposition. Furthermore, the best-fit packing method does not\nprevent tokens from different documents from appearing in training sequences, whereas our method\ndoes. The presence of padding tokens in best-fit packed sequences also means that some context is\nwasted during each optimization step.\nE\nTraining stability with VSL and curriculum\n presents the stability-efficiency dilemma: efficient LLM pretraining with massive data parallelism\nresults in a large batch size and requires a high learning rate. However, such a setup can result in\ntraining instability, leading to poor generalization. They observe a correlation between training\ninstability and long sequences, especially at the early stages of training, suggesting that training on\nlong sequences when the model is not well-trained can be a main source of training instability.\nHere, we show that dataset decomposition alleviates this problem when used with a curriculum:\nstarting training by sampling more from short sequence buckets. We empirically demonstrate this\nby training an OpenLM-1B model from scratch with a high learning rate (= 10−2) and no gradient\nclipping, once with baseline-8k and once with DD using the \"Grow-P100\" curriculum. Training loss\nis shown in Fig. 8, demonstrating the stability of training with DD in comparison to the baseline.\nThis suggests that our proposed method can also be beneficial for large-scale pretraining with large\nbatches and high learning rates in terms of efficiency.\nLoss\nBaseline-8k\nDD-Grow-P100\nFigure 8: We compare the training loss when training with Baseline-8k versus DD with the \"GrowP100\" curriculum. Both models are trained with identical hyperparameters, a high learning rate\n(= 10−2), and no gradient clipping. It is evident that DD results in greater stability.\nF\nAverage sequence length vs average context length\nWe compute the mean of length (Fig. 3a) and context (Fig. 3c) distributions as follows. Assume\na list of sequences with lengths l1, l2, . . . , lN, which are, for example, the chunk lengths in the\nconcat-and-chunk approach or the sequence lengths in different buckets of the dataset decomposition\napproach. We define the average sequence length as follows:\nAverage sequence length = 1\nN\nN\nX\ni\nli\n(1)\nIn auto-regressive training on a sequence with length l, we apply l losses for next-token prediction on\neach token in parallel. Hence, for a sequence with length l, we see contexts with lengths equal to\n20\n\n\n[[PAGE 21]]\n0, 1, 2, . . . , l −1. We define the average context length, which is different from the average sequence\nlength, as follows:\nAverage context length =\n\n\nN\nX\ni=1\nli−1\nX\nj=0\nj\n\n/\nN\nX\ni=1\nli\n!\n=\nN\nX\ni=1\nli(li −1)\n!\n/\n2\nN\nX\ni=1\nli\n!\n.\n(2)\nIn Fig. 3a, Fig. 3c, and Table 1, we report the average sequence length and average context length for\noriginal documents, concat-and-chunk, and dataset decomposition with different mixtures.\n21\n\n\n[[PAGE 22]]\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: Method proposed in Section 2.1 and experiments provided in Section 3 support\nall claims in the abstract.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We provide limitations in Section 5.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [NA]\nJustification: Our work does not have any theorem/proof.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n22\n\n\n[[PAGE 23]]\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We provide all experimental details in Appendix B. Further, we will be\nreleasing the code.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: Our results are based on OpenLM repository (https://github.com/\nmlfoundations/open_lm) and RefinedWeb  data, both publicly available. We\nwill be releasing our (small) changes to the repository after acceptance.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\n23\n\n\n[[PAGE 24]]\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental Setting/Details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the",
      "start_page": 4,
      "end_page": 20,
      "images": [
        {
          "id": "img_p6_1_99b67464e74e.png",
          "page": 6,
          "bbox": [
            108.0,
            75.6299057006836,
            230.76026916503906,
            197.322021484375
          ],
          "caption": null,
          "path": "data/outputs\\images\\img_p6_1_99b67464e74e.png",
          "metadata": {
            "xref": null
          }
        },
        {
          "id": "img_p6_2_16926274f5f3.png",
          "page": 6,
          "bbox": [
            233.47500610351562,
            79.97493743896484,
            368.11859130859375,
            197.322021484375
          ],
          "caption": null,
          "path": "data/outputs\\images\\img_p6_2_16926274f5f3.png",
          "metadata": {
            "xref": null
          }
        },
        {
          "id": "img_p6_3_11d270ad5a16.png",
          "page": 6,
          "bbox": [
            370.8290100097656,
            72.00029754638672,
            501.50836181640625,
            197.322021484375
          ],
          "caption": null,
          "path": "data/outputs\\images\\img_p6_3_11d270ad5a16.png",
          "metadata": {
            "xref": null
          }
        },
        {
          "id": "img_p6_xref72_16926274f5f3.png",
          "page": 6,
          "bbox": null,
          "caption": null,
          "path": "data/outputs\\images\\img_p6_xref72_16926274f5f3.png",
          "metadata": {
            "xref": 72
          }
        },
        {
          "id": "img_p6_xref71_99b67464e74e.png",
          "page": 6,
          "bbox": null,
          "caption": null,
          "path": "data/outputs\\images\\img_p6_xref71_99b67464e74e.png",
          "metadata": {
            "xref": 71
          }
        },
        {
          "id": "img_p6_xref77_11d270ad5a16.png",
          "page": 6,
          "bbox": null,
          "caption": null,
          "path": "data/outputs\\images\\img_p6_xref77_11d270ad5a16.png",
          "metadata": {
            "xref": 77
          }
        },
        {
          "id": "img_p17_1_2152ef1c0d0c.png",
          "page": 17,
          "bbox": [
            108.0,
            71.99935913085938,
            504.0054931640625,
            153.52197265625
          ],
          "caption": null,
          "path": "data/outputs\\images\\img_p17_1_2152ef1c0d0c.png",
          "metadata": {
            "xref": null
          }
        },
        {
          "id": "img_p17_xref106_2152ef1c0d0c.png",
          "page": 17,
          "bbox": null,
          "caption": null,
          "path": "data/outputs\\images\\img_p17_xref106_2152ef1c0d0c.png",
          "metadata": {
            "xref": 106
          }
        },
        {
          "id": "img_p20_1_beda83b61ced.png",
          "page": 20,
          "bbox": [
            192.92730712890625,
            412.4887390136719,
            424.3031311035156,
            510.9576416015625
          ],
          "caption": null,
          "path": "data/outputs\\images\\img_p20_1_beda83b61ced.png",
          "metadata": {
            "xref": null
          }
        },
        {
          "id": "img_p20_xref124_beda83b61ced.png",
          "page": 20,
          "bbox": null,
          "caption": null,
          "path": "data/outputs\\images\\img_p20_xref124_beda83b61ced.png",
          "metadata": {
            "xref": 124
          }
        }
      ],
      "metadata": {
        "bullets": [
          "2.1 Dataset decomposition Given a dataset D of tokenized documents {d1, d2, .",
          ", dn}, the goal of dataset decomposition (DD) is to reorganize D as a union of buckets, ∪iDi, such that: (1) each bucket Di consists of sequences of tokens with length li; (2) each sequence s ∈Di is a subsequence of one document d ∈D; and (3) each token in D appears in exactly one Di.",
          "Dataset decomposition as defined above is not unique.",
          "We propose a specific decomposition, with li = 2i, to optimally maintain the original document sequence length distribution while also enabling efficient batch pretraining, as explained in Section 2.2.",
          "We apply decomposition at the document level, which makes it very easy to integrate the method into any existing data preparation pipeline (a stage before model training) and is scalable to large datasets.",
          "For a tokenized document d ∈D with length l, where l = 2i1 + 2i2 + ."
        ]
      }
    }
  ],
  "metadata": {}
}